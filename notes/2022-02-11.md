
## Intro
- 2022-02-11T17:00:00-04:00
- Received this file: `S&P500 -10ks (2008-2020).xlsx`
- And these details:
  - > Attached is a file with links to all the S&P500 companies’ 10K filings to the SEC from 2008 – 2020.
    > 
    > Here are the steps for getting the data we need for analysis:
    > 
    > 1. Open the sec-link
    > 2. click on the 10K link (it’s usually the first one on the list of documents filed to the SEC from the company for that year)
    > 3. search for the following two terms in the 10K filing:  “diverse”, “diversity”
    > 4. record the instances in column K
    > 5. pull the sentences that include the terms “diverse” and “diversity” and add it to columns starting in column L. For a filing that has one instance, there will just be one sentence in column L. For the filings that include two sentences, pull sentence one into column L and second sentence into column L. 
    > 
    > Notice how I changed the sentences from rows (as we discussed earlier) to columns. What do you think? If you think we should record in separate rows, let me know and I can update the steps.
  - > Do you remember sharing with me on the first day of class your love for coding? And I said don’t tempt me…well you have and I have a project a student of mine, Kai, is trying to work through. 😊 At this stage, it’s a fairly manual process, but it does seem like there are a series of logic commands that could be used by someone with the right skills to ease the pain.
    > 
    > Would you be interesting in helping? If so, I have the excel file attached and the steps written out by Kai (see below). At glance, does this look feasible?
    > 
    > Happy to jump on a call to talk through if helpful. Of course, I understand if it’s not as straightforward as I’m thinking or if you simply do not have the bandwidth. 

## File Analysis

### Intro
- Filetype: XSLX (Excel XML Spreadsheet)
- Alignment: Row-Wise
- Incomplete-Lines: Yes
- Rows: 4239
- Columns: 
- Column-Keys-Brief: `Cnt`, `Gvkey`, `docid`, `CIK`, `CONAME`, `FDATE`, `RDATE`, `FORM`, `sec_link`, `conm`, `"diverse" or "diversity" instance`, `Sentence - 1`, `Sentence - 2`, `Sentence - 3`, `Sentence - 4`, `Sentence - 5`

### Column-Keys-Details
```yaml
# Key: !type [range]
Cnt:          !integer  [1,337]
Gvkey:        !integer  [0,999999]
docid:        !integer  [0,9999999999]
CIK:          !integer  [0,9999999999]
CONAME:       !string   any
FDATE:        !isodate  [2000-01-01,2022-02-10]
RDATE:        !isodate  [2000-01-01,2022-02-10]
FORM:         !string   "10-K"
sec_link:     !url      https://www.sec.gov/Archives/edgar/data/edgar/data/
conm:         !string   any
"diverse" or "diversity" instance: !integer [0,UNKNOWN]
Sentence - 1: !string   any
Sentence - 2: !string   any
Sentence - 3: !string   any
Sentence - 4: !string   any
Sentence - 5: !string   any
```

### Example-Candidates
- Example-Candidate-1: 
  - Excel-File-Row:   1
  - Directory-Link:   https://www.sec.gov/Archives/edgar/data/edgar/data/66740/0001104659-08-011226-index.htm
  - Plain-HTML-Link:  https://www.sec.gov/Archives/edgar/data/66740/000110465908011226/a08-2453_110k.htm
- Example-Candidate-2:
  - Excel-File-Row:   4239
  - Directory-Link:   https://www.sec.gov/Archives/edgar/data/edgar/data/1555280/0001555280-21-000098-index.htm
  - XBRL-Link:        https://www.sec.gov/ix?doc=/Archives/edgar/data/1555280/000155528021000098/zts-20201231.htm
  - Plain-HTML-Link:  https://www.sec.gov/Archives/edgar/data/1555280/000155528021000098/zts-20201231.htm

### Example-Complete-Field
```yaml
Cnt:      1
Gvkey:    007435
docid:    14884
CIK:      0000066740
CONAME:   3M CO
FDATE:    2008-02-15
RDATE:    2007-12-31
FORM:     10-K
sec_link: https://www.sec.gov/Archives/edgar/data/edgar/data/66740/0001104659-08-011226-index.htm
conm:     3M CO
"diverse" or "diversity" instance: 1
Sentence - 1: Investments in property, plant and equipment enable growth in diverse markets, helping to meet product demand and increasing manufacturing efficiency.
Sentence - 2:
Sentence - 3:
Sentence - 4:
Sentence - 5:
```

### Example-Directory-Source-Code
```html
<!-- https://www.sec.gov/Archives/edgar/data/edgar/data/66740/0001104659-08-011226-index.htm -->
<div style="padding: 0px 0px 4px 0px; font-size: 12px; margin: 0px 2px 0px 5px; width: 100%; overflow:hidden">
  <p>Document Format Files</p>
  <table class="tableFile" summary="Document Format Files">
      <tbody><tr>
        <th scope="col" style="width: 5%;"><acronym title="Sequence Number">Seq</acronym></th>
        <th scope="col" style="width: 40%;">Description</th>
        <th scope="col" style="width: 20%;">Document</th>
        <th scope="col" style="width: 10%;">Type</th>
        <th scope="col">Size</th>
      </tr>
      <tr>
        <td scope="row">1</td>
        <td scope="row">10-K</td>
        <td scope="row"><a href="/Archives/edgar/data/66740/000110465908011226/a08-2453_110k.htm">a08-2453_110k.htm</a></td>
        <td scope="row">10-K</td>
        <td scope="row">3815518</td>
      </tr>
      ...
  </tbody></table>  
</div>
```

### Notes 2
- Only some documents are "iXBRL HTML"
- All "iXBRL" appear to have a "plain HTML" available
  - The link to the plain html is not available from the page-source (`link` part of anchor does not exist at load-time)
    - The consequence of this is needing to open the webpage in a browser, and then grabbing the link from the now-loaded content
      - Or...
        - The paths in the url's seem pretty similar
          - Can just trim the url down to the plain html one
- Why only "diverse" and "diversity"? Why not "inclusivity" or other words?
  - What is the intent? Why these words?
  - Should "diversify" and "diversified" be excluded?
  - What are synonyms for "diverse"?
- Some of these files are just regular HTML
  - Others have some insane markup applied to them
  - "Regular HTML" is also a stretch. It seems these are just Microsoft-Word HTML exports (resultingly really, really bad HTML)
    - going to need to look at tools to mitigate this horrendous mess

## Anticipated Programmer Steps
- Export Excel to CSV
- Write "10-K Holder" Class (Python)
- Write Task Class (Python) (Regex) (Wget)
- Write CSV Loader
- Eh....

## Anticipated Program Steps
- Check Installs (Modules)
- Read Configs (Variables)
- Load File for Row Streaming?
- (Maybe better to just dump)
- Convert Full xlsx to reduced "stage 1" csv
- download directory page
- grab 10-k document link
- convert to original plain-html link
- write to "stage 2" csv
- download document
- strip html
- join on newline
- split on punctuation
- build list from regex for words
- count list
- write to "stage 3" csv
- write to xlsx

## Data Format 1

### Notes
- Ok, so this is interesting
- Apparently the SEC took the time to markup the filings with some kind of semantics
- They actual-format seems to be something similar to RecFiles
- (I converted it to YAML for this example)
- What is interesting is in the HTML itself it uses some non-html elements
- https://www.sec.gov/ixviewer/js/lib/he.js
- https://www.sec.gov/ixviewer/js/production.min.js?d=11-2-2022
- "XBRL"
- [SEC Adopts Final Rules on XBRL: Mandates use of interactive data format for public company financial statements](https://www.dwt.com/insights/2009/02/sec-adopts-final-rules-on-xbrl-mandates-use-of-int)
- [Inline XBRL](https://www.sec.gov/structureddata/osd-inline-xbrl.html)
- [Standard Taxonomies](https://www.sec.gov/info/edgar/edgartaxonomies.shtml)
- [xBRL US SEC Reporting](https://xbrl.us/home/filers/sec-reporting/)
- [Wikipedia: XBRL](https://en.wikipedia.org/wiki/XBRL)

### HTML Comments
```html
<div id="dynamic-xbrl-form" class="position-relative">
<!--XBRL Document Created with Wdesk from Workiva-->
<!--p:9ae94d39a76b489097115aacb1d1ed99,x:2b82f9ae3fcd4677b7866fd2ebb1afe5-->
<!-- Document created using Wdesk  -->
<!-- Copyright 2020 Workiva -->
```

### HTML Element
```html
<span>
  <ix:nonfraction 
    id="fact-identifier-197" 
    name="trv:ChangeInNetUnrealizedGainLossOnInvestmentSecuritiesHavingCreditLossesRecognizedInConsolidatedStatementOfIncomeBeforeTaxPortionAttributableToParent" 
    contextref="FD2018Q4YTD" 
    unitref="usd" 
    decimals="-6" 
    scale="6" 
    sign="-" 
    format="ixt:numdotdecimal" 
    inside-table="true" 
    data-original-id="d16544927e1770-wk-Fact-D8A6232FF5C256908390B5AC11846DE5" 
    continued-taxonomy="false" 
    enabled-taxonomy="true" 
    highlight-taxonomy="false" 
    selected-taxonomy="false" 
    hover-taxonomy="false" 
    onclick="Taxonomies.clickEvent(event, this)" 
    onkeyup="Taxonomies.clickEvent(event, this)" 
    onmouseenter="Taxonomies.enterElement(event, this);" 
    onmouseleave="Taxonomies.leaveElement(event, this);" 
    tabindex="18" 
    isadditionalitemsonly="false" 
    iscustomonly="true"
  >27</ix:nonfraction>
</span>
```

### YAML Object
```yaml
# Other Comprehensive Income (Loss), Available-for-sale Securities Adjustment, before Tax, Portion Attributable to Parent
Attributes:
  Tag : us-gaap:EarningsPerShareDiluted
  Fact : 9.92
  Period : 12 months ending 12/31/2019
  Measure : USD / SHARES
  Scale : Zero
  Decimals : Hundredths
  Sign : Positive
  Type : Per Share Item Type
  Format : numdotdecimal

Labels:
  Documentation : The amount of net income (loss) for the period available to each share of common stock or common unit outstanding during the reporting period and to each share or unit that would have been outstanding assuming the issuance of common shares or units for all dilutive potential common shares or units outstanding during the reporting period.
  Label : Earnings Per Share, Diluted
  Terse Label : Diluted (in dollars per share)
  Verbose Label : Net income per common share, diluted (in dollars per share)

References:
- Name : Accounting Standards Codification
  Paragraph : 2
  Publisher : FASB
  Section : S99
  Sub Topic : 10
  Sub Paragraph : (SX 210.5-03(21))
  Topic : 220
  URL (Will Leave SEC Website) : http://asc.fasb.org/extlink&oid=116634182&loc=SL114868664-224227

- Name : Accounting Standards Codification
  Paragraph : 11
  Publisher : FASB
  Section : 50
  Sub Topic : 10
  Topic : 250
  URL (Will Leave SEC Website) : http://asc.fasb.org/extlink&oid=109234566&loc=d3e22694-107794

- Name : Accounting Standards Codification
  Paragraph : 2
  Publisher : FASB
  Section : 45
  Sub Topic : 10
  Topic : 260
  URL (Will Leave SEC Website) : http://asc.fasb.org/extlink&oid=117326831&loc=d3e1252-109256

- Name : Accounting Standards Codification
  Paragraph : 60B
  Publisher : FASB
  Section : 45
  Sub Topic : 10
  Sub Paragraph : (d)
  Topic : 260
  URL (Will Leave SEC Website) : http://asc.fasb.org/extlink&oid=117326831&loc=SL5780133-109256

- Name : Accounting Standards Codification
  Paragraph : 7
  Publisher : FASB
  Section : 45
  Sub Topic : 10
  Topic : 260
  URL (Will Leave SEC Website) : http://asc.fasb.org/extlink&oid=117326831&loc=d3e1337-109256

- Name : Accounting Standards Codification
  Paragraph : 1
  Publisher : FASB
  Section : 50
  Sub Topic : 10
  Sub Paragraph : (a)
  Topic : 260
  URL (Will Leave SEC Website) : http://asc.fasb.org/extlink&oid=6371337&loc=d3e3550-109257

- Name : Accounting Standards Codification
  Paragraph : 52
  Publisher : FASB
  Section : 55
  Sub Topic : 10
  Topic : 260
  URL (Will Leave SEC Website) : http://asc.fasb.org/extlink&oid=117327953&loc=d3e4984-109258

- Name : Accounting Standards Codification
  Paragraph : 1
  Publisher : FASB
  Section : S99
  Sub Topic : 220
  Sub Paragraph : (SX 210.9-04(23))
  Topic : 942
  URL (Will Leave SEC Website) : http://asc.fasb.org/extlink&oid=116637391&loc=SL114874048-224260

- Name : Accounting Standards Codification
  Paragraph : 1
  Publisher : FASB
  Section : S99
  Sub Topic : 220
  Sub Paragraph : (SX 210.7-04(19))
  Topic : 944
  URL (Will Leave SEC Website) : http://asc.fasb.org/extlink&oid=116637232&loc=SL114874131-224263

Calculation:
  Balance : Credit
  Section : 2430404 - Disclosure - Consolidating Financial Statements (Details) - Consolidating Statement of Comprehensive Income (Unaudited)
  Weight : Added to parent (1.00)
  Parent : US-GAAPOther Comprehensive Income Loss Before Tax Portion Attributable To Parent
```

## Data Format 2

### XML
- [Candidate-2](view-source:https://www.sec.gov/Archives/edgar/data/1555280/000155528021000098/zts-20201231_htm.xml)
- > Inline Document  zts-20201231.htm  
  > Custom Taxonomy    
  > ZTS Schema  zts-20201231.xsd  
  > ZTS Label  zts-20201231_lab.xml  
  > ZTS Calculation  zts-20201231_cal.xml  
  > ZTS Presentation  zts-20201231_pre.xml  
  > ZTS Definition  zts-20201231_def.xml  

### iXBRL
- [Candidate-2](https://www.sec.gov/ix?doc=/Archives/edgar/data/1555280/000155528021000098/zts-20201231.htm)
- [Candidate-2](view-source:https://www.sec.gov/ix?doc=/Archives/edgar/data/1555280/000155528021000098/zts-20201231.htm)

### Plain HTML
- [Candidate-2](https://www.sec.gov/Archives/edgar/data/1555280/000155528021000098/zts-20201231.htm)

## Misc Resources
- [Removing HTML Tags from a text file](https://stackoverflow.com/questions/43395942/removing-html-tags-from-a-text-file)
- [html2text is a Python script](https://github.com/aaronsw/html2text) [(Modern Fork)](https://github.com/Alir3z4/html2text/)
- [In Python 3, how to apply html2text() better when the html includes markdown grammar](https://stackoverflow.com/questions/41298542/in-python-3-how-to-apply-html2text-better-when-the-html-includes-markdown-gra)
- [Download a working local copy of a webpage [closed]](https://stackoverflow.com/questions/6348289/download-a-working-local-copy-of-a-webpage)
- [Python equivalent of a given wget command](https://stackoverflow.com/questions/24346872/python-equivalent-of-a-given-wget-command)
- [How can i export html to file via command line with FireFox](https://stackoverflow.com/questions/15429745/how-can-i-export-html-to-file-via-command-line-with-firefox)
- [Download large file in python with requests](https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests?noredirect=1&lq=1)
- [How to download a full webpage with a Python script?](https://stackoverflow.com/questions/31205497/how-to-download-a-full-webpage-with-a-python-script)
- [Scrapy is a fast high-level web crawling and web scraping framework](https://github.com/scrapy/scrapy)
- [Beautiful Soup is a Python library for pulling data out of HTML and XML files](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Download only the text from a webpage content in Python](https://stackoverflow.com/questions/30951657/download-only-the-text-from-a-webpage-content-in-python)
- [BeautifulSoup Grab Visible Webpage Text](https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text)
- [Scrapy at a glance](https://docs.scrapy.org/en/latest/intro/overview.html)
- [Command line tool](https://docs.scrapy.org/en/latest/topics/commands.html)
- [Your Guide to Reading Excel (xlsx) Files in Python](https://www.marsja.se/your-guide-to-reading-excel-xlsx-files-in-python/)
- [openpyxl is a Python library to read/write Excel](https://foss.heptapod.net/openpyxl/openpyxl)
- [How to extract the first or last line from a multi-line cell in Excel?](https://www.extendoffice.com/documents/excel/5864-excel-extract-first-line-of-cell.html)
- [How to check if a module is installed in Python and, if not, install it within the code?](https://stackoverflow.com/questions/44210656/how-to-check-if-a-module-is-installed-in-python-and-if-not-install-it-within-t)
- [Openpyxl, Pandas or both](https://stackoverflow.com/questions/70047335/openpyxl-pandas-or-both)
- [Different ways to iterate over rows in Pandas Dataframe](https://www.geeksforgeeks.org/different-ways-to-iterate-over-rows-in-pandas-dataframe/)
- [pass openpyxl data to pandas](https://stackoverflow.com/questions/36655525/pass-openpyxl-data-to-pandas)
- [Append existing excel sheet with new dataframe using python pandas](https://stackoverflow.com/questions/38074678/append-existing-excel-sheet-with-new-dataframe-using-python-pandas)
- [how to append columns in existing excel sheet using panda in python](https://stackoverflow.com/questions/43135047/how-to-append-columns-in-existing-excel-sheet-using-panda-in-python)
- ["Large data" workflows using pandas [closed]](https://stackoverflow.com/questions/14262433/large-data-workflows-using-pandas?rq=1)
- [Pandas or openpyxl?](https://www.reddit.com/r/inventwithpython/comments/981wwz/pandas_or_openpyxl/)
- [Working with Pandas and NumPy](https://openpyxl.readthedocs.io/en/latest/pandas.html)
- [Loading Ridiculously Large Excel Files in Python](https://towardsdatascience.com/loading-ridiculously-large-excel-files-in-python-44ba0a7bea24)
- [Python - appending to a pickled list](https://stackoverflow.com/questions/28077573/python-appending-to-a-pickled-list)
- [How to use append with pickle in python?](https://stackoverflow.com/questions/12761991/how-to-use-append-with-pickle-in-python)
- [jsonpickle is a library for the two-way conversion of complex Python objects and JSON.](https://github.com/jsonpickle/jsonpickle)
- [How to read a CSV file from a stream and process each line as it is written?](https://stackoverflow.com/questions/6556078/how-to-read-a-csv-file-from-a-stream-and-process-each-line-as-it-is-written)
- [Simple CSV Data Wrangling with Python](https://medium.com/district-data-labs/simple-csv-data-wrangling-with-python-3496aa5d0a5e)
- [Can I stream a Python pickle list, tuple, or other iterable data type?](https://stackoverflow.com/questions/17623523/can-i-stream-a-python-pickle-list-tuple-or-other-iterable-data-type)
- [Find the last row from a CSV input Python](https://stackoverflow.com/questions/53483389/find-the-last-row-from-a-csv-input-python) (`readlines()[-1]` gets last line, very useful!)
- [Python inheritance: Concatenating with super `__str__`](https://stackoverflow.com/questions/31062946/python-inheritance-concatenating-with-super-str)
- [Padding Strings in Python](https://stackabuse.com/padding-strings-in-python/)
- [Extending base classes in Python](https://stackoverflow.com/questions/33534/extending-base-classes-in-python)

## Final Packaging 1
- 1 Python File, Here-File Format
  - Code License Block (MIT)
  - Data License Block (CC0)
  - Readme Block (Markdown)
  - Arch Block (Markdown)
  - Install Section (Python) (Requirements.txt, Auto-Pip)
  - Configs Section (Python)
  - Definitions Section (Python)
  - Input Block (CSV)
  - Results Block (CSV)
  - Namespace Check (Python)

## Final Packaging 2
- Excel File, Original Layout with New Data
- Readme File, Basic Setup Information, Licensing, Context, Debugging
- Python File, Definitions, Variables, Program

## What is This Thing Called
- It's not a **Company**
- It's not a **10-K Form Document**
- It's not a full **Company Statistic** Metadata Document
- It's some **Arbitrary Graph Relation Snippet**
- It's some **One-Off Company Fact**
- It's some **Particular Company 10-K Analysis**

## Importance of Versioning Data
- How important is this?
- Should some `script_version` column be included with the data to indicate the accuracy of the data?
- Or some `updated` column with an ISODateTimeStamp?

## Local or Streaming
- Should all the files be downloaded off the Internet?
- Or should they only be streamed as needed?

## Handling Incompleteness
- Note how there are several rows with data missing
- How to catch these and indicate to the user?

## Receipts
- What logs/receipts, if any, should the program produce?
- What format should be provided for a non-technical user?

## Definitions 1
```python
class CompanyDocumentAnalysis():
  """
  comments missing to clarify this only works on the SEC website for a limited time only
  """
  def __init__(
    self,
    # (Original, Existential) On the original file, row id is implicit but ultimately uniquely distinguishes each item
    row_id:         int               = None,
    # (Original, Arbitrary) unknown, artifact from original file
    cnt_id:         int               = None,
    # (Original, Arbitrary) unknown, artifact from original file
    gvkey:          GVKey             = None,
    # (Original, Arbitrary) unknown, artifact from original file
    docid:          DocId             = None,
    # (Original, Arbitrary) unknown, artifact from original file
    cik:            CIK               = None,
    # (Original, Arbitrary) unknown, artifact from original file
    company_name:   str               = None,
    # (Original, Arbitrary) unknown, artifact from original file
    f_date:         LimitedIsoDateStr = None,
    # (Original, Arbitrary) unknown, artifact from original file
    r_date:         LimitedIsoDateStr = None,
    # (Original, Arbitrary) which type of document to look for
    form_name:      str               = None,
    # (Original, Arbitrary) where the document can be found
    directory_url:  str               = None,
    # (Original, Arbitrary) unknown, artifact from original file
    company_name_2: str               = None,
    # (Original, Arbitrary) how many instances of the words were found
    word_count:     int               = None,
    # (Original, Arbitrary) which sentences were found with the words
    sentences:      list              = None,
    # (Custom) where the document can be found
    document_link:  ArchiveDocLink    = None,
    # (Custom) the regex to use for finding the words
    search_regex:   str               = None,
    # (Custom) where the save file is
    save_file_name: str               = None,
    # (Custom) has this been saved yet?
    saved:          bool              = None
  ):
    self.row_id = row_id
    self.cnt_id = cnt_id
    self.gvkey = gvkey
    self.docid = docid
    self.cik = cik
    self.company_name = company_name
    self.f_date = f_date
    self.r_date = r_date
    self.form_name = form_name
    self.directory_url = directory_url
    self.company_name_2 = company_name_2
    self.word_count = word_count
    self.sentences = sentences
    self.document_link = document_link
    self.search_regex = search_regex
    self.save_file_name = save_file_name
    self.saved = saved
  def complete(self):
    if not self.row_id: raise ValueError("Row Id missing, Cannot assign data to an untracked object")
    if not self.form_name: raise ValueError("Form Name missing, cannot determine what type of document to find")
    if not self.directory_url: raise ValueError("Directory URL missing, cannot determine where to find document link")
    if not self.search_regex: raise ValueError("Search Regex missing, must choose words to look for")
    if not self.save_file_name: raise ValueError("Save File Name missing, must set where to save results")
    if not self.document_link:
      # do steps to get document link
      pass

# actually, this is handled by simply instantiating the object (is handled by the init)
# makes me wonder how many others (and how much of this) is completely unnecessary to begin with (inappropriate lack of delegation of authority)
# (maybe it would be better if everything was cast in the init? but then again, not everything might be known at init-time)
# (why should this class care? Don't init if not ready!)
# ALTERNATIVELY, could try casting in the "complete" method, as a class-init-method should be able to read an object and make that same object
#     if not self.document_link.valid():
#       # do steps to correct the document link
#       pass

    if not self.word_count:
      # do steps to get word count
      # also sets sentences
      # compile regex here
      pass
    if not isinstance(self.sentences,list): raise ValueError("Sentence List has not been set, something has gone wrong, please inspect `CompanyDocumentAnalysis.complete()`")
    if not self.saved:
      # use openpyxl to edit the original xlsx
      pass

class GVKey(int):
  def __str__(self):
    return(super().__str__().rjust(6,'0'))

class DocId(int):
  pass

class CIK(int):
  def __str__(self):
    return(super().__str__().rjust(10,'0'))

class LimitedIsoDateStr():
  def __init__(self,date_str:str):
    self.year, self.month, self.day = date_str.split("-")
    if self.year > 2020: raise ValueError("Date Year too high, please ensure correct data in use")
    if self.year < 2007: raise ValueError("Date Year too low, please ensure correct data in use")
    if self.month < 0 or self.month > 12: raise ValueError("Invalid month")
    if self.day < 0 or self.day > 31: raise ValueError("Invalid day")
  def __str__(self):
    return("-".join(self.year, self.month, self.day))

class ArchiveDocLink():
  def __init__(self,link:str):
    # if the link does not look like an "old" 'archive' link, try to fix it
    pass
```

## Data Native or Format Native
- Or, to prefer Pandas (Data Transformability) or OpenPyxl
- Pandas appears to have limited excel-append options
- OpenPyxl has more robust in-place modification options
- But OpenPyxl needs special code to be written to use it
- Whereas Pandas can be far more generically used
- But cannot easily be appended to an existent excel file

## Intermediate Forms
- Should Any "Intermediate Form" Files be generated?
- "Plan" files (indicate what is going to be done next)
- "Result" files (instead of keeping the excel file open for both reading and writing)

## Streaming
- Alternatively, what if a simple "queue" were built up of "things to get next"
- And then just open the file for writing (and occasionally for reading the next item)
- Could then implement as a scanner of some kind
- Though this depends on how OpenPyxl likes to work (read in full object at once)
- May be worth dumping to a temporary python pickle instead
- Use CSV instead
- Multiple kinds of CSV per file? Nah.... Use two separate CSV's

## MultiStage CSV
- Can indicate with leading bit on each line
```csv
stage,row_id,arbitrary_data
1,1,https://directory.link/12345
...
2,1,https://document.link/12345
...
3,2,5,"first sentence with diverse\nsecond sentence with diversity"
```
- Can then quickly scan for items with `f"\n{stage},{row}"`
- probably call something like `.temp.csv` and delete on finish
- makes very obvious and quick to determine what step is next
- kinda-sorta stateless

## Definitions 1 Comments
- This is a bad frame
- (Many of these variables are shared)
- Should be moved to its own class instead (one level higher - "Project")
- Most of these values unnecessary - only care about the `directory_url` and `row_id` really
- Should be sparse to avoid conflicts
- (Hmmm, do we really need even this?)
- Alternatively, to what extent do we really care about a single object?
- Why not just make everything part of a bulk update operation?
- (Probably best option)
- Alternatively, to what extent do we care about in-place modification?
- Why not return a list of new types each time?
- (Ick, terrible idea)
- Or of writing the statuses/qualities of an item as tags in a list
- (Better but still bad - best to indicate type of an item by its actual physical nature)

## Definitions 2
```python
# Set Variables Up-Front
form_name = "10-K"
search_regex = r""
save_file_name = "/path/to/savefile.xlsx"
source_file_name = "/path/to/sourcefile.xlsx"

def main():
  # load source
  # write sparse csv1 (directory urls)
  # write sparse csv2 (document urls)
  pass

if __name__ == "__main__":
  main()
```

## (Offtopic) Expressing Empty List
- How to best do this?
- Or rather, in Python, how to check that something is a list and not nothing
- (Has to do with implied semantics and structure)
- Consider that one cannot write `if not myList` (as `[] == None`)
- Consider that one can write `if myList is None` (though this is less obvious what exactly we want)
- Consider that one might then write `if not isinstance(myList,list)` to be absolutely clear about what is wanted/intended

## (Offtopic) Delegating Object Property Responsibility
- See Misc/2022-02-11.dinnocenzo/notes.md
- Was just thinking about where to put the details of an object
- (Specifically, of the parameters to some object)
- Can try and capture details through type-declarations of said parameters
- (Or catch with casting)
- But made me wonder, if each parameter has special qualities about it that are needed
- And these qualities might not want to be tested immediately, but checked later
- Then it might be best to leave that inspection-code in-line in some code if it is only used once
- Alternatively, it might be wrapped up into its own tiny class
- and then to give context, that class should be declared within the larger class
- but this can get quite crowded
- and what of throwing errors?
- should each parameter define its own kinds of exceptions?
- just how much detail is enough detail?
- what of subclassing as well?

## 

## 

## 



